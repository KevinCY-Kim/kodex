# KevinCY-Kodex — Testing Strategy

**Version:** 2025.11  
**Scope:** FastAPI · AI/RAG · Clean Architecture

---

## 1. Testing Philosophy

The testing philosophy of `KevinCY-Kodex` is **"to continuously deliver reliable code."** All code must prove its quality through tests.

-   **Quality Assurance**: Guarantees that new features work as intended.
-   **Regression Prevention**: Prevents existing features from breaking due to code changes.
-   **Confident Refactoring**: Allows for confident code structure improvements, as tests act as a safety net.
-   **Living Documentation**: Well-written test code serves as a specification for the feature itself.

---

## 2. Test Types and Scope

The project manages quality centered around three types of tests.

1.  **Unit Test**
    -   **Scope**: A single function, method, or class.
    -   **Characteristics**: All external dependencies (DB, APIs, AI models, etc.) are **mocked**. Runs in a very fast and isolated environment.
    -   **Goal**: To verify the correctness of the logic.

2.  **Integration Test**
    -   **Scope**: Tests the interaction of two or more layers. (e.g., `Service` + `Repository`).
    -   **Characteristics**: May include interaction with a real DB (a test DB) or external systems. Slower than unit tests.
    -   **Goal**: To verify the interaction and data flow between layers.

3.  **E2E Test (End-to-End Test)**
    -   **Scope**: Tests the entire flow from an API endpoint request to the response, identical to a real user scenario.
    -   **Characteristics**: The slowest, but provides the most certain guarantee of the entire system's operation.
    -   **Goal**: To verify the stability of the entire system and its behavior from the end-user's perspective.

---

## 3. Layer-by-Layer Strategy

Each layer of the Clean Architecture is tested according to the following strategy.

### 3.1 `/app/routers`
-   **Test Targets**:
    -   Does it return a `200 OK` status code for a valid request?
    -   Does it return `422 Unprocessable Entity` for an invalid request (validation failure)?
    -   Is the `RequestModel` correctly passed to the `Service` layer?
    -   Is the return value from the `Service` correctly formatted into the `ResponseModel`?
-   **Tool**: FastAPI's `TestClient`

### 3.2 `/app/services`
-   **Test Targets**:
    -   All branches of the core business logic (if/else, etc.).
    -   Does it call functions from `Repository` or `AI` modules with the correct arguments?
    -   Does it behave correctly based on the return values from external layers?
    -   Does it handle exceptions correctly?
-   **Tool**: `pytest`, `pytest-mock` (for mocking external layers)

### 3.3 `/app/repositories`
-   **Test Targets**:
    -   Are DB queries correctly written and executed?
    -   Does it make requests to external APIs with the correct endpoint and parameters?
    -   Does it correctly transform responses from the DB or API into the right data objects?
-   **Tool**: `pytest`, `pytest-mock`, `httpx` (for mocking external APIs), test DB session

---

## 4. AI Pipeline Testing Strategy

Considering the non-deterministic nature of AI models, we focus on testing the **structure and format** of the output, not the content itself.

### 4.1 RAG Retrieval Test
-   **Goal**: To verify that for a specific question, the intended core document (Chunk) is included in the top N search results.
-   **Method**: Create predefined pairs of "question-to-required-chunk-ID". After running the `retriever` function, use an `assert` statement to check if the required chunk is present in the results.

### 4.2 LLM Response Generation Test
-   **Goal**: To verify the **format** of the answer generated by the LLM. (The truthfulness of the content is difficult to test).
-   **Method**:
    -   Mock the LLM call to return a predefined text.
    -   Verify that the answer generation function takes this text and returns a valid JSON structure, including required keys (`answer`, `sources`, etc.).

---

## 5. Test Environment & Tools

-   **Test Runner**: `pytest`
-   **Mocking Library**: `pytest-mock`
-   **HTTP Client (for API tests)**: `TestClient` (from `fastapi.testclient`)
-   **HTTP Client (for mocking external APIs)**: `httpx`

---

## 6. Test Execution Rules

1.  All new feature additions and bug fixes must include corresponding test code.
2.  Before merging code into the `main` branch, all tests (`unit`, `integration`) must pass. (Enforced by CI).
3.  Test code must also adhere to all rules in `Code_Style_eng.md`.
4.  It is recommended to maintain a target test coverage of **80% or higher**.

---

End of KevinCY-Kodex Testing Strategy